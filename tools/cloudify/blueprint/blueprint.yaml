########
# Copyright (c) 2018 COEGSS - gogolenko@hlrs.de
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

tosca_definitions_version: cloudify_dsl_1_3

imports:
    # to speed things up, it is possible to download this file,
    - http://raw.githubusercontent.com/mso4sc/cloudify-hpc-plugin/master/resources/types/cfy_types.yaml
    # HPC pluging
    - https://raw.githubusercontent.com/MSO4SC/cloudify-hpc-plugin/master/plugin.yaml

inputs:
    ############################################
    # arguments of the script
    ############################################
    half_similarity:
        description: Half-similarity scale
        default: "5000"
        type: string

    damping:
        description: Damping
        default: "0"
        type: string

    stripe_size:
        description: Percentage of the sample for the similarity calculation
        default: "0.1"
        type: string

    ############################################
    # Details of the application lifecycle
    ############################################
    # First HPC configuration
    coegss_hlrs_hazelhen:
        description: Configuration for the primary HPC to be used
        default: {}

    # Second HPC configuration
    coegss_psnc_eagle:
        description: Configuration for the secondary HPC to be used
        default: {}

    # Third HPC configuration
    coegss_hlrs_vulcan:
        description: Configuration for the secondary HPC at HLRS to be used
        default: {}

    coegss_sn4sp_repository_clone:
        description: Path to CoeGSS git repository clone
        default: '~/SN4SP'
        type: string

    python_module:
        description: Path to Python virtual environment. @TODO It will be replaced by the name of the module that must be loaded on the target HPC to get access to Python 2.7 environment with libraries that network reconstruction script dependences on.
        default: '~/opt/pyenv/coegss/2.7'
        type: string

    num_processes:
        description: Number of MPI processes
        default: "2"
        type: string

    # Monitor
    monitor_entrypoint:
        description: Monitor entrypoint IP
        default: ''
        type: string

    # Job prefix name
    job_prefix:
        description: Job name prefix in HPCs
        default: 'coegss_sn4sp_'
        type: string


    ############################################
    # Data publish
    ############################################
    coegss_psnc_datacatalogue_entrypoint:
        description: entrypoint of the data catalogue
        default: "https://coegss1.man.poznan.pl"

    coegss_hlrs_datacatalogue_key:
        description: API Key to publish the outputs
        default: ""

    coegss_psnc_datacatalogue_key:
        description: API Key to publish the outputs
        default: ""

    coegss_input_dataset:
        description: ID of the CKAN input dataset
        default: ""

    coegss_output_dataset:
        description: ID of the CKAN output dataset
        default: ""

node_templates:
    first_hpc:
        type: hpc.nodes.Compute
        properties:
            config: { get_input: coegss_psnc_eagle }
            # external_monitor_entrypoint: { get_input: monitor_entrypoint }
            job_prefix: { get_input: job_prefix }
            base_dir: "$HOME"
            workdir_prefix: "cloudify_coegss_sn4sp"
            skip_cleanup: True

    preprocessing_job:
        type: hpc.nodes.job
        properties:
            job_options:
                type: 'SBATCH'
                nodes: { get_input: num_processes }
                tasks: { get_input: num_processes }
                tasks_per_node: 1
                max_time: '00:25:00'
                command: 'coegss_preprocess_submit.sh'
            deployment:
                bootstrap: 'scripts/bootstrap_preprocessor.sh'
                revert: 'scripts/revert_preprocessor.sh'
                inputs:
                    - { get_input: half_similarity }                       # Half-similarity Scale
                    - { get_input: damping }                               # Damping Function
                    - { get_input: stripe_size }                           # Percentage of the sample for the similarity calculation
                    - { get_input: num_processes }                         # Number of MPI processes

                    - { get_input: python_module }                         # python virtual environment
                    - { get_input: coegss_sn4sp_repository_clone }         # repository with CoeGSS tools

                    - { get_input: coegss_psnc_datacatalogue_entrypoint }  # CKAN entrypoint
                    - { get_input: coegss_psnc_datacatalogue_key }         # CKAN API key
                    - { get_input: coegss_input_dataset }                  # CKAN input dataset
                    - { get_input: coegss_output_dataset }                 # CKAN output dataset
            # skip_cleanup: False
            skip_cleanup: True
            # publish:
            #       - type: "CKAN"
            #         entrypoint: { get_input: coegss_psnc_datacatalogue_entrypoint }
            #         api_key: { get_input: coegss_psnc_datacatalogue_key }
            #         dataset: { get_input: coegss_output_dataset }
            #         file_path: "${CURRENT_WORKDIR}/Synthetic population_pdd.h5"
            #         name: "Sampled synthetic population"
            #         description: ""
        relationships:
            - type: job_contained_in_hpc
              target: first_hpc

    network_sampling_job:
        type: hpc.nodes.job
        properties:
            job_options:
                type: 'SBATCH'
                nodes: { get_input: num_processes }
                tasks: { get_input: num_processes }
                tasks_per_node: 1
                max_time: '00:25:00'
                command: 'coegss_network_sampler_submit.sh'
            deployment:
                bootstrap: 'scripts/bootstrap_network_sampler.sh'
                revert: 'scripts/revert_network_sampler.sh'
                inputs:
                    - { get_input: half_similarity }                       # Half-similarity Scale
                    - { get_input: damping }                               # Damping Function
                    - { get_input: stripe_size }                           # Percentage of the sample for the similarity calculation
                    - { get_input: num_processes }                         # Number of MPI processes

                    - { get_input: python_module }                         # python virtual environment
                    - { get_input: coegss_sn4sp_repository_clone }         # repository with CoeGSS tools

                    - { get_input: coegss_psnc_datacatalogue_entrypoint }  # CKAN entrypoint
                    - { get_input: coegss_psnc_datacatalogue_key }         # CKAN API key
                    - { get_input: coegss_input_dataset }                  # CKAN input dataset
                    - { get_input: coegss_output_dataset }                 # CKAN output dataset
            # skip_cleanup: False
            skip_cleanup: True
            # publish:
            #       - type: "CKAN"
            #         entrypoint: { get_input: coegss_psnc_datacatalogue_entrypoint }
            #         api_key: { get_input: coegss_psnc_datacatalogue_key }
            #         dataset: { get_input: coegss_output_dataset }
            #         file_path: "${CURRENT_WORKDIR}/Synthetic network.h5"
            #         name: "Synthetic network"
            #         description: "Synthetic network produced by parallel job"
        relationships:
            - type: job_contained_in_hpc
              target: first_hpc
            - type: job_depends_on
              target: preprocessing_job

outputs:
    preprocessing_job_name:
        description: Preprocessing job name in the HPC
        value: { get_attribute: [preprocessing_job, job_name] }
    network_sampling_job_name:
        description: Network sampling job name in the HPC
        value: { get_attribute: [network_sampling_job, job_name] }
